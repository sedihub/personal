{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "The idea is to use the pre-trained GPT2 model to generate text starting with a prompt sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2Model, TFGPT2LMHeadModel\n",
    "from transformers import pipeline, set_seed\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import GPT2 Model and its Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 10:33:53.459732: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-08 10:33:53.460116: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  # \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = TFGPT2Model.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"vocab_size\": 50257,\n",
      "    \"n_positions\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"use_cache\": true,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"return_dict\": true,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_attentions\": false,\n",
      "    \"torchscript\": false,\n",
      "    \"torch_dtype\": null,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"pruned_heads\": {},\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"is_decoder\": false,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"add_cross_attention\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"temperature\": 1.0,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"output_scores\": false,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"finetuning_task\": null,\n",
      "    \"id2label\": {\n",
      "        \"0\": \"LABEL_0\",\n",
      "        \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"label2id\": {\n",
      "        \"LABEL_0\": 0,\n",
      "        \"LABEL_1\": 1\n",
      "    },\n",
      "    \"tokenizer_class\": null,\n",
      "    \"prefix\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"sep_token_id\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"problem_type\": null,\n",
      "    \"_name_or_path\": \"gpt2\",\n",
      "    \"transformers_version\": \"4.27.4\",\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print model configs:\n",
    "print(json.dumps(model.config.to_dict(), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~Generate Text using HF's API~~\n",
    "\n",
    "~~Currently, only the PyTorch model is supported...~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = pipeline('text-generation', model=\"gpt2\", framework=\"pt\")\n",
    "\n",
    "# # Generate text:\n",
    "# set_seed(42)\n",
    "# prompt_text = \"Replace me by any text you'd\"\n",
    "# generator(prompt_text, max_length=15, num_return_sequences=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Next Token Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"Replace me\" # by any text you'd like\"\n",
    "encoded_input = tokenizer(prompt_text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_input[\"input_ids\"].shape)\n",
    "\n",
    "print(output.keys())\n",
    "print(output[\"logits\"].shape)\n",
    "print(output[\"past_key_values\"][0].shape, output[\"past_key_values\"][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(\n",
    "    token_ids=encoded_input[\"input_ids\"][0])\n",
    ")\n",
    "\n",
    "print(output[\"logits\"].shape)\n",
    "greedy_output_tokens = output[\"logits\"].numpy().argmax(axis=-1)\n",
    "print(greedy_output_tokens.shape)\n",
    "print(tokenizer.decode(\n",
    "    token_ids=greedy_output_tokens[0][-1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold = 0.05\n",
    "\n",
    "last_token_probs = tf.nn.softmax(logits=output[\"logits\"][0, -1, :], axis=-1).numpy()\n",
    "last_token_ids = tf.argsort(last_token_probs, axis=-1, direction='DESCENDING').numpy()\n",
    "last_token_ids = last_token_ids[last_token_probs[last_token_ids] >= probability_threshold]\n",
    "\n",
    "# print(last_token_tokens.shape)\n",
    "for token, prob in zip(last_token_ids, last_token_probs[last_token_ids]):\n",
    "    token_str = tokenizer.decode(token_ids=token).replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "    print(f\"\\t[{round(float(prob), 3)}] {token_str}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sequentially using GREEDY search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_generate_length = 10\n",
    "prob_threshold = 1.0e-3\n",
    "prompt_text = \"At least it is a nice day out to enjoy\"\n",
    "initial_prompt_length = len(prompt_text)\n",
    "\n",
    "token_prob = 1.0\n",
    "gen_count = 0\n",
    "while gen_count <= max_generate_length and token_prob > prob_threshold:\n",
    "    # Generate the next token:\n",
    "    encoded_input = tokenizer(prompt_text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    last_token_probs = tf.nn.softmax(logits=output[\"logits\"][0, -1, :], axis=-1).numpy()\n",
    "    token_ids = np.argsort(last_token_probs)\n",
    "    token_prob = last_token_probs[token_ids[-1]]\n",
    "    next_token = tokenizer.decode(token_ids=token_ids[-1])\n",
    "    prompt_text = prompt_text + next_token\n",
    "    # print(\"-> [{}] {}\".format(\n",
    "    #     round(float(token_prob), 3), next_token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    print(\"\\t-> [{:5}] {}\".format(\n",
    "        str(round(float(token_prob), 3)), prompt_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    gen_count += 1\n",
    "\n",
    "# print(f\">>{prompt_text[:initial_prompt_length]}<<\")\n",
    "# print(prompt_text[initial_prompt_length:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sequentially using probabilisitc sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_generate_length = 10\n",
    "stop_prob_threshold = 1.0e-2\n",
    "prompt_text = \"At least it is a nice day out to enjoy\"\n",
    "initial_prompt_length = len(prompt_text)\n",
    "\n",
    "max_token_prob = 1.0\n",
    "gen_count = 0\n",
    "while gen_count <= max_generate_length and max_token_prob > stop_prob_threshold:\n",
    "    # Generate the next token:\n",
    "    encoded_input = tokenizer(prompt_text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    last_token_probs = tf.nn.softmax(logits=output[\"logits\"][0, -1, :], axis=-1).numpy()\n",
    "    token_id = np.searchsorted(\n",
    "        a=np.cumsum(last_token_probs), \n",
    "        v=np.random.uniform(low=0.0, high=1.0, size=(1,)), \n",
    "        side=\"right\", sorter=None)\n",
    "    max_token_prob = last_token_probs.max()\n",
    "    token_prob = last_token_probs[token_id]\n",
    "    #\n",
    "    next_token = tokenizer.decode(token_ids=token_id)\n",
    "    prompt_text = prompt_text + next_token\n",
    "    # print(\"\\t-> [{}] {}\".format(\n",
    "    #     round(float(token_prob), 3), next_token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    print(\"\\t-> [{:5}] {}\".format(\n",
    "        str(round(float(token_prob), 3)), prompt_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    gen_count += 1\n",
    "\n",
    "print(f\"[{prompt_text[:initial_prompt_length]}]{prompt_text[initial_prompt_length:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_generate_length = 200\n",
    "stop_prob_threshold = 1.0e-2\n",
    "prompt_text = \"She always leaves behind a trace of golden fabulousness\"\n",
    "initial_prompt_length = len(prompt_text)\n",
    "\n",
    "max_token_prob = 1.0\n",
    "gen_count = 0\n",
    "while gen_count <= max_generate_length and max_token_prob > stop_prob_threshold:\n",
    "    # Generate the next token:\n",
    "    encoded_input = tokenizer(prompt_text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    last_token_probs = tf.nn.softmax(logits=output[\"logits\"][0, -1, :], axis=-1).numpy()\n",
    "    token_id = np.searchsorted(\n",
    "        a=np.cumsum(last_token_probs), \n",
    "        v=np.random.uniform(low=0.0, high=1.0, size=(1,)), \n",
    "        side=\"right\", sorter=None)\n",
    "    max_token_prob = last_token_probs.max()\n",
    "    token_prob = last_token_probs[token_id]\n",
    "    #\n",
    "    next_token = tokenizer.decode(token_ids=token_id)\n",
    "    prompt_text = prompt_text + next_token\n",
    "    # print(\"\\t-> [{}] {}\".format(\n",
    "    #     round(float(token_prob), 3), next_token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    # print(\"\\t-> [{:5}] {}\".format(\n",
    "    #     str(round(float(token_prob), 3)), prompt_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    gen_count += 1\n",
    "\n",
    "print(f\"[{prompt_text[:initial_prompt_length]}]{prompt_text[initial_prompt_length:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_generate_length = 100\n",
    "stop_prob_threshold = 1.0e-2\n",
    "prompt_text = \"Glad you're having fun\" #\"Tomorrow I will go lift...\"\n",
    "initial_prompt_length = len(prompt_text)\n",
    "\n",
    "max_token_prob = 1.0\n",
    "gen_count = 0\n",
    "while gen_count <= max_generate_length and max_token_prob > stop_prob_threshold:\n",
    "    # Generate the next token:\n",
    "    encoded_input = tokenizer(prompt_text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    last_token_probs = tf.nn.softmax(logits=output[\"logits\"][0, -1, :], axis=-1).numpy()\n",
    "    token_id = np.searchsorted(\n",
    "        a=np.cumsum(last_token_probs), \n",
    "        v=np.random.uniform(low=0.0, high=1.0, size=(1,)), \n",
    "        side=\"right\", sorter=None)\n",
    "    max_token_prob = last_token_probs.max()\n",
    "    token_prob = last_token_probs[token_id]\n",
    "    #\n",
    "    next_token = tokenizer.decode(token_ids=token_id)\n",
    "    prompt_text = prompt_text + next_token\n",
    "    # print(\"\\t-> [{}] {}\".format(\n",
    "    #     round(float(token_prob), 3), next_token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    # print(\"\\t-> [{:5}] {}\".format(\n",
    "    #     str(round(float(token_prob), 3)), prompt_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    gen_count += 1\n",
    "\n",
    "print(f\"[{prompt_text[:initial_prompt_length]}]{prompt_text[initial_prompt_length:]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select from the top 90%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_generate_length = 100\n",
    "stop_prob_threshold = 1.0e-2\n",
    "prompt_text = \"I want to hear about your AI\" \n",
    "initial_prompt_length = len(prompt_text)\n",
    "\n",
    "max_token_prob = 1.0\n",
    "gen_count = 0\n",
    "while gen_count <= max_generate_length and max_token_prob > stop_prob_threshold:\n",
    "    # Generate the next token:\n",
    "    encoded_input = tokenizer(prompt_text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    last_token_probs = tf.nn.softmax(logits=output[\"logits\"][0, -1, :], axis=-1).numpy()\n",
    "    last_token_argsort = np.argsort(-1.0 * last_token_probs)\n",
    "    token_id = last_token_argsort[np.searchsorted(\n",
    "        a=np.cumsum(last_token_probs[last_token_argsort]), \n",
    "        v=np.random.uniform(low=0.0, high=0.6, size=(1,)), \n",
    "        side=\"right\", sorter=None)]\n",
    "    max_token_prob = last_token_probs.max()\n",
    "    token_prob = last_token_probs[token_id]\n",
    "    #\n",
    "    next_token = tokenizer.decode(token_ids=token_id)\n",
    "    prompt_text = prompt_text + next_token\n",
    "    # print(\"\\t-> [{}] {}\".format(\n",
    "    #     round(float(token_prob), 3), next_token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    # print(\"\\t-> [{:5}] {}\".format(\n",
    "    #     str(round(float(token_prob), 3)), prompt_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')))\n",
    "    gen_count += 1\n",
    "\n",
    "print(f\"<{prompt_text[:initial_prompt_length]}>{prompt_text[initial_prompt_length:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
